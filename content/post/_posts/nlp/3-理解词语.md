---
title: 理解词语
categories:
    - nlp
tags:
    - nlp
author: 高山临溪谷
comments: true
---
# 理解词语

## 机器是这样理解语言 - 词向量

### 1. 计算机理解语言

<img 
    src="../../img/blogs/nlp/理解词语/nlp-w2v3.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
<img 
    src="../../img/blogs/nlp/理解词语/nlp-w2v4.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

* 计算机之所以能看懂字里行间的感情，理解文字，处理文字，并不是因为它理解的我们普罗万象的人类语言，而是它将语言或者词汇归类到了一个正确的位置上。计算机对词语的理解，其实是计算机对空间及位置的理解。
* 不管是图片，文章，句子，词语，声音，只要是能被数值化，被投射到某个空间中，计算机都能把它们按相似度聚集起来。
<img 
    src="../../img/blogs/nlp/理解词语/nlp-w2v6.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

### 2. 词向量

<img 
    src="../../img/blogs/nlp/理解词语/nlp-w2v7.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
在词向量训练的过程中，相似词总会被聚集到一块地方且方向大概都相同，比如这里的猫狗龙。而差异较大的词会被拉远，且方向有可能不同，所以和猫狗这些动物相比，没有生命的飞机，碗等都离得很远。

所以如果只想测量两个词的相似度，角度信息也足够了。但点与点的距离还透露了更多的信息，只要两个词总在一起出现，他们之间的关联性应该越强，距离应该也越近。我们想一想，如果一个词不仅出现的频率高，而且任何句子中都能出现，比如“在”，“你”，“吗”这一类的词。

<img 
    src="../../img/blogs/nlp/理解词语/nlp-w2v8.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
为了得到这些词的位置，机器需要不断计算他们之间的相关性。这个过程称之为机器学习或者模型训练。这些词每次训练的时候都想被拉扯到独立的空间，但是被太多不同方向的词拉来拉去，比如”在”这个字，训练“在这”的时候“在”字被拉扯到靠近“这”字的方向。训练“在家”的时候，“在”字将会更靠近“家”字，后面的训练也一样，所以“在”字因为频率太高，和很多字都能混搭，它就算是之中机器认为的“中性词”，越有区分力的词可能越远离中心地带，因为他们和其他词都不像，而越通用，在每种场景都有的词，就可能越靠近原点。这时，点与点的距离就能告诉我们词的频率性特征。

### 3. 训练词向量

<img 
    src="../../img/blogs/nlp/理解词语/nlp-w2v9.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
训练词向量可以直接在原始语料上做非监督学习，只要有各种各样的文章数据就行。训练时，取一小段文本，取出这些词的向量表示，比如取出除了“一”字以外的词向量，然后整合到一起，表示这些文字的整体向量，用这个整体向量预测最中间那个“一”。接下来在开始下一段文字的训练。

将这个窗口挪动一格，用前后文预测“段”字，接着将窗口依次这样扫过所有文字，用所有的前后文预测中间词，这样计算机就能将前后文的关系搞清楚，挨得近的词他们的关系越亲密总出现在类似的上下文中间的词关系越亲密。向量在一定程度上也越相近。除了用前后文预测中间词，我们还能换一个思路，用中间词预测前后文也行。

### 4. 词向量的应用

<img 
    src="../../img/blogs/nlp/理解词语/nlp-w2v10.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
有种用法很简单，就是直接把词向量当成词语特征输入进另一个模型里。这样就能用更丰富的词向量信息来表示一个词语ID。在这种情况中，我们说词向量是一种预训练特征。用word2vec 的方法预先训练好了词语的特征表达，然后在其他场景中拿着预训练结果直接使用。

<div style="display: flex; justify-content: space-between;">
    <img src="../../img/blogs/nlp/理解词语/nlp-w2v11.png" width="50%" />
    <img src="../../img/blogs/nlp/理解词语/nlp-w2v12.png" width="50%" />
</div>
还有种更有趣的玩法，用词向量进行加减运算，男人减掉女人的词向量，差不多就约等于公猫减掉母猫的词向量。

## Continuous Bag of Words (CBOW)

### 1. CBOW模型

用一句话概述：挑一个要预测的词，来学习这个词前后文中词语和预测词的关系。

```举个例子: 我爱莫烦Python，莫烦Python通俗易懂。```

模型在做的事情如图中所示，将这句话拆成输入和输出，用前后文的词向量来预测句中的某个词。
<img 
    src="../../img/blogs/nlp/理解词语/cbow_illustration.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
通过在大数据量的短语或文章中学习这样的词语关系，这个模型就能理解要预测的词和前后文的关系。而图中彩色的词向量就是这种训练过程的一个副产品。

### 2. 词向量的应用

词向量的几种典型应用：

- 把这些对词语理解的向量通过特定方法组合起来，就可以有对某句话的理解了；
- 可以在向量空间中找寻同义词，因为同义词表达的意思相近，往往在空间中距离也非常近；
- 词语的距离换算。
词语距离计算这个比较有意思，比如可以拿词语做加减法。公猫 - 母猫 就约等于 男人 - 女人。 

## Skip-Gram

### 1. Skip-Gram模型
Skip-Gram 是把CBOW过程反过来,使用文中的某个词，然后预测这个词周边的词。
<div style="display: flex; justify-content: space-between;">
    <img src="../../img/blogs/nlp/理解词语/cbow_illustration.png" width="50%" />
    <img src="../../img/blogs/nlp/理解词语/skip_gram_illustration.png" width="50%" />
</div>

* Skip-Gram 相比 CBOW 最大的不同，就是剔除掉了中间的那个 SUM 求和的过程，我在这里提到过, 我觉得将词向量求和的这个过程不太符合直观的逻辑，因为我也不知道这加出来的到底代表着是一个句向量还是一个另词向量，求和是一种粗暴的类型转换。 而Skip-Gram没有这个过程，最终我们加工的始终都是输入端单个词向量，这样的设计我比较能够接受。