---
title: 搜索
categories:
    - nlp
tags:
    - nlp
author: 高山临溪谷
comments: true
---
# 搜索

## 搜索引擎工作原理

### 1. 搜索引擎

<img 
    src="../../img/blogs/nlp/搜索/nlp-search2.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

* 在早期互联网时代，世界上诞生了一个影响全世界的公司，Google，它所带来的搜索也突破老一代的技术，登上了历史舞台，并影响着后续2-30年的人类社会发展。华人比较熟悉的百度则是在稍后一些，为中文搜索带来了前所未有的体验。

### 2. 垂类搜索

<img 
    src="../../img/blogs/nlp/搜索/nlp-search3.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

* 搜个社交，点个外卖，查个账款，旅个游，这些生活中的服务体系，早已经深度嵌入了我们一天中每一个时间点，对应上了会发生在我们生活的每一个行为。当别人在谈论搜索引擎的时候，可能已经不再是谈论google百度这样的搜索入口了，那千千万万个长在垂类应用里的搜索，才是组成我们今天生活获取信息的主要入口。

### 3. 构建索引

<img 
    src="../../img/blogs/nlp/搜索/nlp-search5.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

* 在一篇文章可以被搜索之前，搜索引擎将对他们进行细致的观察，因为它可不想把全部的信息，不分重点的一股脑存储下来。取而代之，它会挑选重点部分，分别对待，比如重点关注标题、时间、正文。将这些信息给予不同的权重后，接着就是下一步，将它存储起来。

* 搜索引擎通常在搜索的时候，不会临时从全网找材料，而是将刚刚收集到的信息提前构建成索引，存储在便于快速检索的数据库中。只在自己的数据库中搜索，使我们的及时搜索更有效率。

### 4. 数值匹配搜索

<img 
    src="../../img/blogs/nlp/搜索/nlp-search6.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

* 现在的深度学习给我们提供了另一条思路，也就是用模型从非文字的信息中提取计算机能够识别的可计算信息。 在用户用文字搜索时，将搜索的文字内容转换成深度学习能识别的数字内容，然后再和之前存储的图片、视频数字信息进行匹配，对比两种数字之间的关联性，然后找到最相近的内容。这种搜索，我们有一个专业名词叫作多模态搜索。

* 多模态搜索并不仅限于文字搜图片视频，它还能颠倒过来，用图片搜图片，图片搜视频等，因为在深度学习看来，只要它们能被转换成统一的数字形态，我就能对比相似性。

### 5. 搜索过滤

<img 
    src="../../img/blogs/nlp/搜索/nlp-search7.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

* 深度学习模型有一个速度的硬伤，相比传统方法，每一次预测需要消耗更多的时间。而用户是无法忍受搜索中的延迟。为了实现在海量网页和文件中的快速搜索遍历，我们不得不使用到更加传统的方法, 而把深度学习方法放到后续更加适合的步骤中。

* 目前比较常用的方式，类似于这里的层层筛选过滤的方式，将筛选结果用不同的方法，从海量的网页中，一层层过滤到最符合你搜索条件的结果。而在需要做大量文档过滤处理的阶段，我们就使用时间消耗相对较少的技术，最后可以把深度学习方案，放在文档量和计算量都少的地方。接下来我们就介绍一下，什么样的技术能在数据量庞大的地方，又快又准地帮你找到搜索内容。

### 6. 正排/倒排索引

<img 
    src="../../img/blogs/nlp/搜索/nlp-search8.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

* 我们总说NLP会让计算机懂得文字的内涵，但是有时候，有更加投机取巧的方法可以让计算机在不理解文字内涵的时候，还能给我们快速带来准确的结果。特别是在搜索中不得不提到的倒排索引技术。倒排索引是一种批量召回技术，它能快速在海量数据中初步召回基本符合要求的文章。

<img 
    src="../../img/blogs/nlp/搜索/nlp-search9.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

假设你开了家咨询公司，手上有100篇材料。这时有人来找你咨询NLP的问题，你会怎么在这100篇材料中找到合适的内容呢？

- 方法1：我们一篇一篇地阅读，找到所有包含NLP内容的材料，然后返回给提问者。这种方法需要我们在每次搜索的时候，都对所有材料进行一次阅读，然后在材料中找到关键词，并筛选出材料，效率其实非常差。

- 方法2：我们在第一次拿到所有材料时，把它们通读一遍，然后构建关键词和文章的对应关系。当用户在搜索特定词的时候，比如“红”，就会直接返回“红”这个【关键词索引】下的文章列表。先构造索引的好处就是能够将这种索引，放在后续的搜索中复用，搜索也就变成了一种词语匹配加返回索引材料的过程。

这里的 方式1是我们所谓的正排索引，方式2是更加快速的倒排索引。但当处理的是海量数据的时候，通过倒排索引找到的文章可能依然是海量。如果能有种方法对这些文章进行排序操作，再选取排名靠前的文章列表也能帮我们节省大量的时间。处理匹配排序，最有名的算法之一叫做TF-IDF。

### 7. TF-IDF

<img 
    src="../../img/blogs/nlp/搜索/nlp-search10.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

先来看看TF-IDF所处的位置是哪里吧，有了批量性地召回相对合适的内容后，比如我已经从1亿个网页中召回了100万个，但100万对于我来说，已经够让我看上好几年了。怎么能再继续提升一下精确度，找到我更在乎的内容呢？

那么对筛选出来的内容做一个【问题与内容】的相似度排序，只返回那些头部内容就好啦。这个工作显然还是有一定的计算量的，所以如果前面不做召回，在1亿个网页中直接用打分排序的逻辑，往往还是挺久的。所以最好是将召回作为初步筛选，然后再相似度打分找到我在乎的内容，从而减轻计算负担。

<img 
    src="../../img/blogs/nlp/搜索/nlp-search11.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
如果说TF是以文章为中心的局部词信息，那么IDF则是全局的词信息。在一篇文章中，越重要的内容，强调的次数也越多，所以**频率**(TF)会大，我们可以用词频高的词代表这篇文章。所以TF可以用一张词和文章标号的表来展示。不过问题来了，像语气词或“你我他”这种词，同样也会出现很多次，光用TF，我们没办法除去这些词的影响。而TF-IDF中的IDF，就可以在这个时候帮上忙，它是所有词在这个系统中的区分力的大小，如果每篇文章里都有“我”这个字，那么它的在任意一篇文章当中的区分力都不强，而如果你关键词搜索的是“莫烦”，那么全网都没有几个叫“莫烦”的，“莫烦”IDF就会很大，意味着“莫烦”的区分力也够强。

<img 
    src="../../img/blogs/nlp/搜索/nlp-search12.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
TF-IDF 两者结合其实就是两者相乘的意思，这样的结果意味着所有的文章，都能用一串集合所有词的分数来表示。通过分数的高低，我们也能大概看出这篇文章的关键内容是什么。

<img 
    src="../../img/blogs/nlp/搜索/nlp-search13.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
假设我们搜索关键词“莫烦Python”,机器会利用词表的模式计算“莫烦Python”这个问题的TF-IDF值。然后会计算问句和每篇文章的cosine距离，这个例子中的计算过程，简单来说，就是将文章按照词的维度放到一个四维空间中，然后把问句同样也放到这个空间里，最后看空间中这个问题离哪一个文章的距离最近，越近则相似度越高。通过这样的方式呢，我们就能找到搜索问题的最佳匹配文章了。

<img 
    src="../../img/blogs/nlp/搜索/nlp-search14.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>
举个例子，第一串数字就是文章1的向量表达，第二串是文章2的向量表达，第三串是问题的向量表达。他们都是空间中的点。

## TF-IDF

### 1. TF-IDF 的数学表达形式

其实它就是一个庞大的矩阵，用词语的数字向量来代表一篇文档，当比较文档时，就是在比较这些向量的相似性。比如下面有三篇文档，每篇文档中都不断重复着莫烦，Python，最棒这三个词。 通过对每个词计算TF-IDF后，我们可以用这这些TF-IDF值代表这三篇文档，也就是说，每篇文档就是一个三维向量。

| 词语   | 文档1 | 文档2 | 文档3 |
|:------:|:-----:|:-----:|:-----:|
| 莫烦   |  0.5  |  0.3  |  0.2  |
| Python |  0.4  |  0.4  |  0.4  |
| 最棒   |  0.1  |  0.3  |  0.4  |

如果把向量展示在图上，就会有类似下面图案的样子，加上一个计算向量相似度的方式，比如cosine距离，我们就能判段哪些文档在这个三维空间上比较相似了。 图中两个蓝色向量离得越近，就代表他们越像。

<img 
    src="../../img/blogs/nlp/搜索/tfidf_doc_vec.png"
    width="500" height="300"
    style="display: block; margin: 0 auto;"
/>

## 搜索的扩展

用TF-IDF来表示文档，然后可以用来做搜索和提取关键词。 可是在代码中存在一个机制，会引发内存占用大的问题。

TF-IDF是一张二维表，分别代表文章索引和单词索引。文章量是可以无限增大的，单词量的增长也是很恐怖的。那么随着这两个维度的增长， 我们的内存总有一天会扛不住。好在我们可以利用一个节约内存的技术，叫做Sparse Matrix，稀疏矩阵，它只会存储有内容的值，而忽略无内容的值。 在这张巨大的二维表中，肯定每篇文章不一定会提及到所有词汇，这些不提及的词汇，我们当然可以不用存储。

用 Skearn 模块的 Sparse Matrix 功能，能更快速，有效地计算和存储海量的数据。